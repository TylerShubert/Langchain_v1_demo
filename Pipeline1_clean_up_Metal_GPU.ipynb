{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f98cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"/Users/m4air/Documents/NVIDIA.pdf\") ##PDF path here## :) \n",
    "documents = loader.load()\n",
    "#text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)#. Defult\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=85) # M4 Macbook Air optimize test 1\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "vectorstore.save_local(\"/Users/m4air/Documents/Machine_Learning/LangchangLLM/output\") ##Output folder path here! ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277fd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cleaned up code with prompt, and setting for apple silicon via metal. ##\n",
    "\n",
    "import os\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain, LLMChain\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Required for Metal optimization (Apple Silicon) \n",
    "os.environ[\"GGML_METAL_NL\"] = \"1\"\n",
    "\n",
    "# PRompt\n",
    "template = \"\"\"You are Robot Mike, a friendly and slightly sarcastic AI assistant. \n",
    "You love helping people and sometimes add a witty comment at the end of your answers.\n",
    "\n",
    "Here is some context to help you answer:\n",
    "{context}\n",
    "\n",
    "Answer the following question in character as Robot Mike:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"question\", \"context\"], template=template)\n",
    "\n",
    "# Load Vectorstore (You MUST define this beforehand) \n",
    "# Example:\n",
    "# from langchain.vectorstores import FAISS\n",
    "# vectorstore = FAISS.load_local(\"your_index_path\", embeddings)\n",
    "## Refrence Langchain documentation ##\n",
    "retriever = vectorstore.as_retriever()  \n",
    "\n",
    "# -- LlamaCpp Config --\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Volumes/Crucial X6/GGUF_llm_models/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\",\n",
    "    n_gpu_layers=30,\n",
    "    n_batch=512,\n",
    "    n_ctx=8192,\n",
    "    f16_kv=True,\n",
    "    n_threads=os.cpu_count(),\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "#  Chain Setup \n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\"\n",
    ")\n",
    "\n",
    "qa = RetrievalQA(\n",
    "    retriever=retriever,\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Ask the Question \n",
    "question = \"Can you summarize this PDF document and give me a general idea of what itâ€™s about? As well as how many times is the man himself jenson mentioned?\"\n",
    "result = qa.invoke({\"query\": question})\n",
    "\n",
    "#print(result[\"result\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
